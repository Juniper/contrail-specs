# 1. Introduction

1.1 Shared Data Center Gateway functionality on Spine (QFX/MX):

Contrail Fabric Manager (CFM) should be able to automate the Gateway functionality from a Data Center fabric. All different Tenants have accessibility to an external routing table where external routes e.g. 0/0 are accessible. This routing table is typically shared across all Tenants within the data center. 
This requirement is already supported in CFM release 5.0.1

1.2 Interconnect between Data Centers:

Contrail Fabric Manager (CFM) should be able to automate the interconnect of two different data centers. Multiple Tenants (Shared VRF) which are connected to a Logical Router using single Shared VRF in given DC should be able to exchange routes with other shared VRF of another DC. It should also be possible to have dedicated tenant should be able to exchange routes with another dedicated VRF on another DC. This requirement is going to be supported in CFM release 5.1. 


# 2. Problem statement
Interconnect of DCs should support following:
- Multiple Tenants Shared VRF on a given DC interconnect with Multiple Tenants Shared VRF on another DC.
- Dedicated Tenant VRF on a given DC interconnect with Dedicated VRF on another DC.
- It is also possible Dedicated Tenant on a given DC should be able to interconnect with Shared VRF on another DC.
- UI should allow user to configure Shared or Dedicated VRF.
- UI should allow user to configure Interconnect of Shared/Dedicated VRFs.
- Control Traffic should be always BGP across devices participating in DCI
- Evpn Type 5 and Inet-VPN interconnect should be supported.
- Supported Encapsulations: MPLS / VXLAN
- It is assumed that devices are already preconfigured (no Greenfield)
- Supported Devices: MX or QFX10K
- Interconnect Devices should be configured with DC-Interconnect-Gateway Role.

### DC Interconnect Shared VRF
Multiple tenants share the Shared VRF.
* Multiple Virtual Networks connected to a Logical Router results into Shared VRF. This logical router is extended to DC-Interconnect Device. 
* Logical Routers should link each other.

### DC Interconnect Dedicated VRF
* Dedicated Tenant's Virtual Network is directly extended to DC-Interconnect Device. 
* Virtual Networks/associated VRFs should link each other.

### DC Interconnect Dedicated VRF with Shared VRF
* Multiple Virtual Networks connected to a Logical Router results into Shared VRF. This logical router is extended to DC-Interconnect Device. 
* Dedicated Tenant's Virtual Network is directly extended to DC-Interconnect Device. 
* Shared VRF and dedicated VRF should link each other.



#### Deployment model: CRB with border spine
Supported Roles:
- DCI-Gateway@spine
- DCI-Gateway@leaf

#### Supported Device Models
Spine is QFX 10K, MX

#### Software Releases
Junos 18.1R3

# 3. Proposed solution

Initially CFM will support interconnect of DCs in a single Fabric. Interconnect of different DCs will be implemented in future releases.
CFM has the capability to push config based assigned roles devices. A new role DCI-Gateway@Spine, DCi-Gateway@leaf is going to be introduced to support this requirement.
Devices participating in DCI interconnect should have role DCI-Gateway. When Devices have this roles, and virtual networks/Logical routers are extended to those devices, the CFM will push right device config to interconnect devices. A new playbook will be added to QFX and Mx. Abstract config generated by device configure will the knobs to identify DCI participating configurations such Encapsulation/Shared VRF VNI/Dedicated VRF VNI.
CFM already has playbooks defined with different roles (for features) to push the config to the DC devices.

## 3.1 Alternatives considered
#### N/A

## 3.2 API schema changes
- There is a need of introduction to DC Interconnect Object in Contrain VNC DB.
- DC Interconnect Object is defined as :
   - Collection of DCs
   - Each DC will have:
      - Gateway PR      (Gateway Device for the DC)
      - Logical Router  (Shared VRF)
      - Virtual Netwrok (Dedicated VRF)
   - DC Interconnect Protocol Configuration attributes (for example: BGP protocol params like : ASN, i/ebgp)
- Extend DC to PRs (participating Spines of DCs), A full mesh of DC interconnect is supported. 
- Currently, A DC maps to a Fabric.
 
## 3.3 User workflow impact
A new device overlay role is added: DCI-Gateway@Spine, DCI-Gateway@Leaf


## 3.4 UI changes
Should be able to establish links between two or more devices participating in DCI.
- Currently, user can create Shared VRF or dedicated VRF in the UI.
- New Requirement: UI should allow create one more Data Center Interconnects (Shared vs Shared, Shared vs Dedicated, Dedicated vs Dedicated). This is not avaiable today. 

## 3.5 Notification impact
#### N/A

# 4. Implementation
## 4.1 Add roles to node profile
CFM uses the node profile object to determine the roles applicable for a type of device. We will add the `DCI-Gateway` role for MX, QFX10k

## 4.2 Add DCI features and associate them with the corresponding roles
- CLI Configuration for DCI (Shared/Dedicated VRf interconnect). This config should be generated jinja template.

DCI-1 Spine config :
 
regress@dc-pc-elit-03> show configuration routing-instances VRF-501 | display inheritance no-comments
instance-type vrf;
interface irb.2001;
interface irb.2002;
interface irb.2003;
interface irb.2004;
interface lo0.501;
route-distinguisher 192.186.0.2:501;
vrf-import VRF-501-IMPORT;
vrf-export VRF-501-EXPORT;
vrf-target {
    import target:200:501;
    export target:100:501;
}
routing-options {
    rib VRF-501.inet6.0 {
        multipath;
    }
    multipath;
}
protocols {
    evpn {
        ip-prefix-routes {
            advertise direct-nexthop;
            encapsulation vxlan;
            vni 200501;                                                                  <<<<< VNI should match with other DCI spine
            export ExportHostRoutes;
        }
    }
}
 
regress@dc-pc-elit-03> show configuration policy-options policy-statement ExportHostRoutes | display inheritance no-comments
term 1 {
    from {
        protocol evpn;
        route-filter 0.0.0.0/0 prefix-length-range /32-/32;
    }
    then accept;
}
term 2 {
    from {
        family inet6;
        protocol evpn;
        route-filter 0::0/0 prefix-length-range /128-/128;
    }
    then accept;
}
term 3 {
    from protocol direct;
    then accept;
}
 
 
Spine config from DC1-2:
 
 
regress@dc-pc-elit-07> show configuration routing-instances VRF-501 | display inheritance no-comments   
instance-type vrf;
interface irb.2001;
interface irb.2002;
interface irb.2003;
interface irb.2004;
interface lo0.501;
interface ae5.1;
route-distinguisher 192.168.0.4:501;
vrf-import VRF-501-IMPORT;
vrf-export VRF-501-EXPORT;
vrf-target {
    import target:100:501;
    export target:200:501;
}
routing-options {
    rib VRF-501.inet6.0 {
        multipath;
    }
    multipath;
}
protocols {
    evpn {
        ip-prefix-routes {
            advertise direct-nexthop;
            encapsulation vxlan;
            vni 200501;
            export [ ExportHostRoutes V4-EXPORT-INTERNET V6-EXPORT-INTERNET ];
        }
    }
}
                                        
{master:0}
regress@dc-pc-elit-07> show configuration policy-options policy-statement ExportHostRoutes | display inheritance no-comments
term 1 {
    from {
        protocol evpn;
        route-filter 0.0.0.0/0 prefix-length-range /32-/32;
    }
    then accept;
}
term 2 {
    from {
        family inet6;
        protocol evpn;
        route-filter 0::0/0 prefix-length-range /128-/128;
    }
    then accept;
}
term 3 {
    from protocol direct;
    then accept;
}
 

# 5. Performance and scaling impact
## 5.1 API and control plane
#### N/A

## 5.2 Forwarding performance
#### N/A

# 6. Upgrade
#### Backward compatible changes

# 7. Deprecations
#### N/A

# 8. Dependencies
- Current physical and routing bridging role lookup and assiginment schema
- Current overlay abstract config generation in DM
- Current overlay config push architecture (Ansible way)

# 9. Testing
## 9.1 Unit tests
The current CFM & DM Ansible config push implementation does not have any unit tests or a unit test infrastructure.
The unit tests for this feature will be in the ansible layer but with the assumption that building the infrastructure won't be in the scope of this feature.

#### Test #1 - Verify Shared VRF Interconnect
- Assign DCI-Gateway roles to participating interconnect devices. (MX or QFX10K)
- Extend one or more virtual networks to Logical Router on DC1.
- Extend one or more virtual networks to Logical Router on DC2.
- Extend DCIs to associated devices.
- Jinja template should push DCI config to devices. 

#### Test #2 - Verify Dedicated VRF Interconnect
- Assign DCI-Gateway roles to participating interconnect devices. (MX or QFX10K)
- Associate a virtual network to DC1.
- Associate another virtual network to DC2.
- Extend DCIs to associated devices.
- Jinja template should push DCI config to devices. 

## 9.2 Dev tests
## 9.3 System tests
#### Test #1 -  Verify Shared VRF Interconnect, QFX10K
- Setup two QFX10K as SPines with Junos 18.1R3
- Setup two VNs on each Device.
- Setup Logical Routers on each
- Attach VNs to associated LRs.
- Create DCI with DC objects
- Extend DCIs to PRs
- Attach BMSs  
- Verify DCI routing

#### Test #2 -  Verify Dedicated VRF Interconnect, MX
- Setup two MX as SPines with Junos 18.1R3
- Setup a VN on each device
- Create DCI with DC objects
- Attach VNs to DCs.
- Extend DCIs to PRs
- Attach BMSs  
- Verify DCI routing

#### Test #2 -  Verify Dedicated VRF vs Shared VRF Interconnect, MX
- Setup two MX as SPines with Junos 18.1R3
- Setup a VN on one device participating in DC1
- Setup two VNs on another Device participating in DC2
- Attach VNs to LR  in DC2.
- Extend DCIs to PRs
- Attach two BMSs
- Verify DCI routing

# 10. Documentation Impact


# 11. References
